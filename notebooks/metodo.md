Me parece que o método utilizado para analisar um conjunto de dados é uma incógnita na área de ciência de dados. O que percebo é que se tomam vários pressupostos a priori que não necessariamente são verdadeiros para problemas específicos, e por outro lado se ingnoram uma série de conhecimentos a respeito dos problemas analisados que são importantes para a solução de tais problemas. O método para analisar/prever/classificar/etc um problema específico envolvendo ciência de dados, me parece transcorrer desta forma:
- É feita uma análise bibliográfica para saber quais são os algoritmos mais utilizados para analisar um determinado problema
- Escolhe-se um algoritmo dentre os mais utilizados que será melhorado
- Adicionam-se um ou mais parâmetros ao algoritmo escolhido para dar maior flexibilidade ou altera-se o próprio algoritmo com foco em maior performance
- Busca-se treinar o algoritmo alterando os parâmetros manualmente até se encontrar um conjunto de parâmetros que proporciona uma taxa de acerto alta
- É feito um processo semelhante com outros algoritmos da área, e o algoritmo sem o parâmetro, mas nessa etapa, geralmente não é documentado quanto se alterou os parâmetros, o que torna a prova de conceito arbitrária
- Se usa o algoritmo encontrado em um caso do mundo real, seja para aumentar os lucros de uma empresa, seja para criar um novo produto que irá ter um atrativo. Esse passo nem sempre é feito, pois o objetivo geralmente é publicar o mais rápido possível. Isso faz com que a real validação seja a posteriori do estudo científico no mais das vezes, quando é feita

Esse método tem uma série de problemas. O que vejo como mais grave é a fragmentação entre o problema real a ser solucionado e a estratégia de análise/interpretação dos dados. Em alguns artigos, essa cisão não é tão proeminente, mas em outros, a solução acontece totalmente em ambiente simulado, e não há a menor intenção de utilizar esta mesma solução no ambiente real. Depois de publicado o artigo, no mais das vezes o trabalho cai no esquecimento.
Para além disso, geralmente há uma fragmentação em quem desenvolve os algoritmos matemáticos/estatísticos/redes neurais/etc e quem realmente os utiliza. As bibliotecas mais utilizadas levam em conta implementações de artigos renomados de quem criou bons algoritmos, mas de quem nunca solucionou um problema real com eles.
Por exemplo, no momento em que vamos simular um ruído sobre algum dado, porque não levamos em consideração as especificidades do sensor que o mediu? Existem trabalhos que focam em melhorar sensores específicos, mas será que eles estão sendo usados para solucionar problemas reais? Como podemos colocar como parrâmetros de um modelo, informações a respeito do problema analisado? O ser humano não pode ajudar nessa tarefa? Geralmente têm-se uma subestimação da ação humana na solução de um problema ou na melhora de um modelo. Justamente porque os modelos são criados em laboratório e quase nunca são testados, ou quando o são, é para aumentar o lucro de uma Uber ou Netflix da vida, e não para entender e solucionar as reais necessidades humanas. 
Agora, se a pessoa cientista de dados que se debruça sobre um problema, tiver sempre como preocupação entender a fundo o problema com as pessoas que querem resolvê-lo em primeiro lugar, muito processamento manual e estupido poderia ser evitado. Por exemplo, se quero construir um algoritmo para prever o volume de uma colheita e não falar em nenhum momento com os trabalhadores e trabalhadoras que tem um conhecimento sobre a colheita, meu trabalho é no mínimo incompleto. 
O ato de criar, treinar e refinar um algoritmo para solução de um problema real, nada mais é do que a automatização de uma tarefa que um especialista poderia fazer. Então é preciso encontrar este especialista, extrair todo o conhecimento dele ou dela, e incorporar isso no código à priori. Isso com certeza auxiliará na escolha do algoritmo a ser utilizado e também no refinamento e ajustes.

O que me parece, é que como há quase sempre uma divisão entre a criação do algoritmo e o processo de solução de um problema real, há uma tendência de se criarem algoritmos mais genéricos e não específicos. Isso faz com que seja cada vez mais difícil determinar o algoritmo a ser utilizado para a solução de novos problemaa. 