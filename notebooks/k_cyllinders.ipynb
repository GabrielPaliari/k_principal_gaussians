{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando a curva para o digito: 0\n",
      "Número de segmentos:  136\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 12.93256s\t lower bound -20324906.81863.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 13.04088s\t lower bound -20324140.14444.\n",
      "Criando a curva para o digito: 1\n",
      "Número de segmentos:  83\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 4.15442s\t lower bound -6351977.57800.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 8.03858s\t lower bound -6343669.76379.\n",
      "Criando a curva para o digito: 2\n",
      "Número de segmentos:  164\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 15.07085s\t lower bound -30740559.40487.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 15.12295s\t lower bound -30737653.02365.\n",
      "Criando a curva para o digito: 3\n",
      "Número de segmentos:  158\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 15.58051s\t lower bound -27486352.10715.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 18.99287s\t lower bound -27487487.85196.\n",
      "Criando a curva para o digito: 4\n",
      "Número de segmentos:  155\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 13.47111s\t lower bound -25401755.24765.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 11.26730s\t lower bound -25401589.75017.\n",
      "Criando a curva para o digito: 5\n",
      "Número de segmentos:  150\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 11.80168s\t lower bound -24211174.27209.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 12.09307s\t lower bound -24213463.17769.\n",
      "Criando a curva para o digito: 6\n",
      "Número de segmentos:  132\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 8.55563s\t lower bound -18287079.21529.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 8.82099s\t lower bound -18286722.78551.\n",
      "Criando a curva para o digito: 7\n",
      "Número de segmentos:  141\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 11.29738s\t lower bound -20600950.38095.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 8.50720s\t lower bound -20603594.77181.\n",
      "Criando a curva para o digito: 8\n",
      "Número de segmentos:  156\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 12.29571s\t lower bound -25889766.34060.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 12.43924s\t lower bound -25889600.50785.\n",
      "Criando a curva para o digito: 9\n",
      "Número de segmentos:  130\n",
      "Initialization 0\n",
      "Initialization converged. time lapse 6.99707s\t lower bound -16802008.29269.\n",
      "Initialization 1\n",
      "Initialization converged. time lapse 8.44597s\t lower bound -16802029.83907.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def gaussian_mixture_k_segs(data, n_components=5):\n",
    "    \"\"\"\n",
    "    Approximate a principal curve using a Gaussian Mixture Model.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: np.ndarray, shape (n_samples, n_features)\n",
    "      Input data.\n",
    "    - n_components: int\n",
    "      Number of Gaussian components in the GMM.\n",
    "    - n_points: int\n",
    "      Number of points to sample on the principal curve.\n",
    "\n",
    "    Returns:\n",
    "    - curve: np.ndarray, shape (n_points, n_features)\n",
    "      The computed principal curve.\n",
    "    \"\"\"\n",
    "    gmm = BayesianGaussianMixture(\n",
    "        n_components=n_components,\n",
    "        max_iter=100,\n",
    "        covariance_type=\"full\",\n",
    "        weight_concentration_prior_type=\"dirichlet_process\",\n",
    "        tol=1e-4,\n",
    "        random_state=2,\n",
    "        verbose=2,       \n",
    "        init_params=\"k-means++\",\n",
    "        n_init=2,\n",
    "        warm_start=True\n",
    "    )\n",
    "    gmm.fit(data)\n",
    "    means = gmm.means_\n",
    "    covariances = gmm.covariances_\n",
    "    return means, covariances\n",
    "\n",
    "def create_segs_from_model(means, covariances):\n",
    "    \"\"\"\n",
    "    Cria os segmentos e as direções de segunda maior variação a partir das médias e covariâncias\n",
    "    \"\"\"\n",
    "    pcs_ortogonal = []\n",
    "    segments = []\n",
    "    for i, mean in enumerate(means):\n",
    "        # Compute PCA on the covariance matrix\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(covariances[i])\n",
    "        \n",
    "        # Ordenar os índices dos valores em ordem decrescente\n",
    "        sorted_vals_idx = np.argsort(eigenvalues)[::-1]\n",
    "        sorted_vecs_idx = np.argsort(eigenvectors)[::-1]\n",
    "        \n",
    "        '''\n",
    "        O multiplicador de confiança é um metaparâmetro que multiplica o tamanho de cada segmento.\n",
    "        É equivalente a multiplicar o desvio padrão local por 1, 2, etc para se elevar a confiança\n",
    "        Daria para determinar este número  iterativamente, fazendo-se uma média do comprimento dos segmentos versus ligações e buscando minimizar a diferença \n",
    "        '''\n",
    "        confidence_multiplier = 2\n",
    "        \n",
    "        # Pegue os dois maiores autovalores\n",
    "        val_index1, val_index2 = sorted_vals_idx[:2]\n",
    "        pc_1_norm, pc_2_norm = confidence_multiplier*np.sqrt(eigenvalues[val_index1]), confidence_multiplier*np.sqrt(eigenvalues[val_index2])\n",
    "        \n",
    "        vec_index1, vec_index2 = sorted_vecs_idx[:2]\n",
    "        pc_1_dir, pc_2_dir = eigenvalues[vec_index1], eigenvalues[vec_index2]\n",
    "                \n",
    "        segment = np.array([mean + pc_1_dir * pc_1_norm, mean - pc_1_dir * pc_1_norm])  \n",
    "        ortogonal = np.array([mean + pc_2_dir * pc_2_norm, mean - pc_2_dir * pc_2_norm])  \n",
    "  \n",
    "        # Save ellipse data\n",
    "        segments.append(segment)\n",
    "        pcs_ortogonal.append(ortogonal)\n",
    "    return segments, pcs_ortogonal\n",
    "\n",
    "def order_segments(segments, ortogonal_components):\n",
    "    '''\n",
    "    Ordena Segmentos e também a componente principal ortogonal  \n",
    "    '''\n",
    "    # Lista para armazenar a ordem dos segmentos\n",
    "    ordered_segments = []\n",
    "\n",
    "    # Usar o primeiro segmento como ponto inicial\n",
    "    current_segment = segments[0]\n",
    "    ordered_segments.append(current_segment)\n",
    "    ordered_pcs = [ortogonal_components[0]]\n",
    "    current_start, current_end = current_segment\n",
    "\n",
    "    # Criar uma lista dos segmentos restantes\n",
    "    remaining_segments = list(segments[1:])\n",
    "    remaining_pcs = list(ortogonal_components[1:])\n",
    "\n",
    "\n",
    "    while remaining_segments:\n",
    "        # Criar uma lista com todos os extremos dos segmentos restantes\n",
    "        candidates = []\n",
    "        for seg in remaining_segments:\n",
    "            candidates.extend(seg)\n",
    "        candidates = np.array(candidates)\n",
    "        \n",
    "        # Calcular a menor distância entre o ponto final do segmento atual e os candidatos\n",
    "        distances_end = cdist([current_end], candidates)\n",
    "        distances_start = cdist([current_start], candidates)\n",
    "        min_idx_end = np.argmin(distances_end)\n",
    "        min_dist_end = np.min(distances_end)\n",
    "        min_idx_start = np.argmin(distances_start)\n",
    "        min_dist_start = np.min(distances_start)\n",
    "        \n",
    "        if min_dist_start > min_dist_end:  \n",
    "            next_segment_idx = min_idx_end // 2\n",
    "            next_segment = remaining_segments[next_segment_idx]\n",
    "            ordered_pcs.append(remaining_pcs[next_segment_idx])\n",
    "            if min_idx_end % 2 == 0:\n",
    "                current_end = next_segment[1] \n",
    "                ordered_segments.append(next_segment)\n",
    "            else:\n",
    "                current_end = next_segment[0] \n",
    "                ordered_segments.append([next_segment[1], next_segment[0]])\n",
    "                \n",
    "        else:\n",
    "            next_segment_idx = min_idx_start // 2\n",
    "            next_segment = remaining_segments[next_segment_idx]\n",
    "            ordered_pcs.insert(0, remaining_pcs[next_segment_idx])\n",
    "            if min_idx_start % 2 == 0:\n",
    "                current_start = next_segment[1] \n",
    "                ordered_segments.insert(0, [next_segment[1], next_segment[0]])\n",
    "            else:\n",
    "                current_start = next_segment[0] \n",
    "                ordered_segments.insert(0, next_segment)\n",
    "            \n",
    "        # Remover o segmento já utilizado\n",
    "        del remaining_segments[next_segment_idx]\n",
    "        del remaining_pcs[next_segment_idx]\n",
    "\n",
    "    return np.array(ordered_segments), ordered_pcs\n",
    "\n",
    "def calc_connection_seg(seg_prev, pc_prev, seg_current, pc_current):\n",
    "    pc_norm = (np.linalg.norm(pc_current) + np.linalg.norm(pc_prev)) / 2\n",
    "    return {\n",
    "        'max_dist': pc_norm,\n",
    "        'seg_points': np.array([seg_prev[1], seg_current[0]]),\n",
    "        'is_conn': True\n",
    "    }   \n",
    "    \n",
    "def extract_final_curve(ordered_segments, ordered_pcs):\n",
    "    final_curve = []\n",
    "    for i, segment in enumerate(ordered_segments):                \n",
    "        final_curve.append({\n",
    "            'ortogonal_pc': ordered_pcs[i],\n",
    "            'seg_points': segment,\n",
    "            'is_conn': False\n",
    "        }) \n",
    "        if (i>=1 and i < len(ordered_segments)):\n",
    "            conn_seg = calc_connection_seg(ordered_segments[i-1], ordered_pcs[i-1], segment, ordered_pcs[i])        \n",
    "            final_curve.append(conn_seg)\n",
    "    return final_curve\n",
    "        \n",
    "'''\n",
    "Aqui, uma ideia interessante seria usarmos a distância de Mahalanobis,\n",
    "utilizando os dados estatísticos estimados pelo gausian mixture para cada segmento.\n",
    "Talvez dê uma medida mais exada da proximidade de um dado a uma curva/segmento e melhore a performance\n",
    "'''    \n",
    "def calc_min_distances(points, segments):\n",
    "    \"\"\"\n",
    "    Calculate the minimum distance from each point to multiple segments and store the segment index.\n",
    "\n",
    "    Parameters:\n",
    "    - points: Array of shape (n, d), where each row is a point in N-dimensional space.\n",
    "    - segments: Array of shape (m, 2, d), where each segment is defined by two points.\n",
    "\n",
    "    Returns:\n",
    "    - min_distances: Array of shape (n,), where each entry is the minimum distance for a point.\n",
    "    - segment_indices: Array of shape (n,), where each entry is the index of the closest segment.\n",
    "    \"\"\"\n",
    "    points = np.array(points)  # Shape: (n, d)\n",
    "    segments = np.array(segments)  # Shape: (m, 2, d)\n",
    "\n",
    "    a = segments[:, 0, :]  # Start points of segments, shape: (m, d)\n",
    "    b = segments[:, 1, :]  # End points of segments, shape: (m, d)\n",
    "\n",
    "    # Vector from A to B (segment direction vectors), shape: (m, d)\n",
    "    ab = b - a\n",
    "\n",
    "    # Squared length of each segment, shape: (m,)\n",
    "    ab_len_sq = np.sum(ab**2, axis=1)\n",
    "\n",
    "    # Expand points to shape (n, m, d)\n",
    "    p_exp = points[:, np.newaxis, :]  # Shape: (n, 1, d)\n",
    "    a_exp = a[np.newaxis, :, :]  # Shape: (1, m, d)\n",
    "    ab_exp = ab[np.newaxis, :, :]  # Shape: (1, m, d)\n",
    "\n",
    "    # Vector from A to P, shape: (n, m, d)\n",
    "    ap = p_exp - a_exp\n",
    "\n",
    "    # Projection factors (t values), shape: (n, m)\n",
    "    t = np.sum(ap * ab_exp, axis=2) / ab_len_sq\n",
    "\n",
    "    # Clamp t to the range [0, 1]\n",
    "    t = np.clip(t, 0, 1)\n",
    "\n",
    "    # Closest points on the segments, shape: (n, m, d)\n",
    "    closest_points = a_exp + t[:, :, np.newaxis] * ab_exp\n",
    "\n",
    "    # Distances from points to the closest points on each segment, shape: (n, m)\n",
    "    distances = np.linalg.norm(p_exp - closest_points, axis=2)\n",
    "\n",
    "    # Minimum distances and corresponding segment indices\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "\n",
    "    return min_distances\n",
    "\n",
    "def gaussian_mixture_principal_curve(data, k):\n",
    "    final_curve = []\n",
    "    means, covariances = gaussian_mixture_k_segs(data, k)\n",
    "    segments, pcs_ortogonal = create_segs_from_model(means, covariances)\n",
    "    ordered_segments, ordered_pcs = order_segments(segments, pcs_ortogonal)\n",
    "    final_curve = extract_final_curve(ordered_segments, ordered_pcs)\n",
    "    curve_segments = np.array([seg['seg_points'] for seg in final_curve])\n",
    "    return curve_segments, final_curve  \n",
    "\n",
    "import pandas as pd\n",
    "file_path = '../data/digit-recognizer/train.csv' \n",
    "digits_images = pd.read_csv(file_path)\n",
    "\n",
    "curves = [];\n",
    "pcas = []\n",
    "for digit in range(10):\n",
    "    print(f\"Criando a curva para o digito: {digit}\")\n",
    "    class_data = digits_images[digits_images[\"label\"] == digit]\n",
    "    pca = PCA(n_components=0.98, random_state=2)\n",
    "    data_reduced = pca.fit_transform(class_data.to_numpy()[:, 1:])\n",
    "    pcas.append(pca)\n",
    "    pca2 = PCA().fit(data_reduced)\n",
    "    cumulative_variance = np.cumsum(pca2.explained_variance_ratio_)\n",
    "    n_components_95 = (np.argmax(cumulative_variance >= 0.98) + 1)\n",
    "    print(\"Número de segmentos: \", n_components_95)\n",
    "    curve_segments, final_curve = gaussian_mixture_principal_curve(data_reduced, n_components_95)\n",
    "    curves.append((curve_segments, final_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando menor distância: 100%|\u001b[32m███████████████\u001b[0m| 10/10 [00:12<00:00,  1.21s/it]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "''' Validação '''\n",
    "num_samples = 1000\n",
    "validation = digits_images.head(num_samples).copy()\n",
    "validation_np = validation.to_numpy()[:, 1:]\n",
    "from tqdm import tqdm\n",
    "\n",
    "for curve_idx in tqdm(range(len(curves)),desc=\"Calculando menor distância\", ncols=80, colour=\"green\"): \n",
    "    segments, metadata = curves[curve_idx] # ainda não usamos a segunda componente principal\n",
    "    pca = pcas[curve_idx]\n",
    "    min_dists = calc_min_distances(pca.transform(validation_np), segments)\n",
    "    column_name = curve_idx\n",
    "    validation[column_name] = min_dists\n",
    "\n",
    "columns_of_interest = [idx for idx in range(10)]\n",
    "validation.loc[:, \"guess\"] = validation[columns_of_interest].idxmin(axis=1)\n",
    "sum_correct_all_columns = validation.loc[validation[\"label\"] == validation[\"guess\"]]\n",
    "print(sum_correct_all_columns.shape[0] / num_samples * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando menor distância:   0%|\u001b[32m                        \u001b[0m| 0/10 [01:37<?, ?it/s]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m segments, metadata \u001b[38;5;241m=\u001b[39m curves[curve_idx] \u001b[38;5;66;03m# ainda não usamos a segunda componente principal\u001b[39;00m\n\u001b[0;32m      9\u001b[0m pca \u001b[38;5;241m=\u001b[39m pcas[curve_idx]\n\u001b[1;32m---> 10\u001b[0m min_dists \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_min_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m column_name \u001b[38;5;241m=\u001b[39m curve_idx\n\u001b[0;32m     12\u001b[0m test_data[column_name] \u001b[38;5;241m=\u001b[39m min_dists\n",
      "Cell \u001b[1;32mIn[10], line 204\u001b[0m, in \u001b[0;36mcalc_min_distances\u001b[1;34m(points, segments)\u001b[0m\n\u001b[0;32m    201\u001b[0m closest_points \u001b[38;5;241m=\u001b[39m a_exp \u001b[38;5;241m+\u001b[39m t[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m ab_exp\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Distances from points to the closest points on each segment, shape: (n, m)\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_exp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclosest_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Minimum distances and corresponding segment indices\u001b[39;00m\n\u001b[0;32m    207\u001b[0m min_distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(distances, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\src\\data_analysis\\venv\\lib\\site-packages\\numpy\\linalg\\_linalg.py:2562\u001b[0m, in \u001b[0;36m_norm_dispatcher\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2558\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(svd(y, compute_uv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 2562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_norm_dispatcher\u001b[39m(x, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x,)\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_norm_dispatcher)\n\u001b[0;32m   2567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm\u001b[39m(x, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' Gerar submissão '''\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = '../data/digit-recognizer/test.csv' \n",
    "test_data = pd.read_csv(file_path)\n",
    "data_points = test_data.to_numpy()\n",
    "for curve_idx in tqdm(range(len(curves)),desc=\"Calculando menor distância\", ncols=80, colour=\"green\"): \n",
    "    segments, metadata = curves[curve_idx] # ainda não usamos a segunda componente principal\n",
    "    pca = pcas[curve_idx]\n",
    "    min_dists = calc_min_distances(pca.transform(data_points), segments)\n",
    "    column_name = curve_idx\n",
    "    test_data[column_name] = min_dists\n",
    "\n",
    "columns_of_interest = [idx for idx in range(10)]\n",
    "test_data.loc[:, \"Label\"] = test_data[columns_of_interest].idxmin(axis=1)\n",
    "answer = test_data[[\"Label\"]].copy()  \n",
    "answer[\"ImageId\"] = range(1, len(answer) + 1)  \n",
    "answer = answer[[\"ImageId\", \"Label\"]]\n",
    "\n",
    "# Save to CSV\n",
    "output_file = \"submission.csv\"\n",
    "answer.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code to generate synthetic data\n",
    "'''\n",
    "# def generate_data_gaussians():    \n",
    "#     # Configurações das gaussianas\n",
    "#     np.random.seed(42)  # Para reprodutibilidade\n",
    "#     num_gaussians = 5\n",
    "#     points_per_gaussian = 1000\n",
    "\n",
    "#     # Parâmetros de cada gaussiana (média e covariância)\n",
    "#     means = [\n",
    "#         [0, 0],\n",
    "#         [5, 3],\n",
    "#         [10, 2],\n",
    "#         [15, 1],\n",
    "#     ]\n",
    "\n",
    "#     covariances = [\n",
    "#         [[2, 3], [0, 0.5]],\n",
    "#         [[5,0], [0.2, 0.5]],\n",
    "#         [[3, -2], [0, 0]],\n",
    "#         [[1.2, 0.6], [0.6, 1]],\n",
    "#     ]\n",
    "\n",
    "#     # Gerar os dados\n",
    "#     data = []\n",
    "#     for mean, cov in zip(means, covariances):\n",
    "#         gaussian_points = np.random.multivariate_normal(mean, cov, points_per_gaussian)\n",
    "#         data.append(gaussian_points)\n",
    "\n",
    "#     data = np.vstack(data)\n",
    "\n",
    "#     # Adicionar ruído\n",
    "#     noise = np.random.normal(0, 0.2, data.shape)\n",
    "#     data_with_noise = data + noise\n",
    "#     return data_with_noise\n",
    "\n",
    "# def load_spiral_data():\n",
    "#     import pandas as pd \n",
    "#     file_path = '../data/spiral_data.csv'  # Update the path if necessary\n",
    "#     spiral_data = pd.read_csv(file_path)\n",
    "\n",
    "#     # Extract x and y coordinates\n",
    "#     x_data = spiral_data['x']\n",
    "#     y_data = spiral_data['y']\n",
    "#     data_with_noise = np.column_stack((x_data, y_data))\n",
    "#     return data_with_noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
