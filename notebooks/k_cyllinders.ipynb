{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def gaussian_mixture_k_segs(data, n_components=5):\n",
    "    \"\"\"\n",
    "    Approximate a principal curve using a Gaussian Mixture Model.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: np.ndarray, shape (n_samples, n_features)\n",
    "      Input data.\n",
    "    - n_components: int\n",
    "      Number of Gaussian components in the GMM.\n",
    "    - n_points: int\n",
    "      Number of points to sample on the principal curve.\n",
    "\n",
    "    Returns:\n",
    "    - curve: np.ndarray, shape (n_points, n_features)\n",
    "      The computed principal curve.\n",
    "    \"\"\"\n",
    "    gmm = BayesianGaussianMixture(\n",
    "        n_components=n_components,\n",
    "        max_iter=10,\n",
    "        covariance_type=\"full\",\n",
    "        weight_concentration_prior_type=\"dirichlet_process\",\n",
    "        random_state=2, # Usar como hyper pa\n",
    "        verbose=2,\n",
    "        init_params=\"k-means++\",\n",
    "        n_init=1,\n",
    "        warm_start=True\n",
    "    )\n",
    "    # gmm = BayesianGaussianMixture(n_components=n_components, covariance_type='full', init_params='k-means++', n_init=10, warm_start=True)\n",
    "    gmm.fit(data)\n",
    "\n",
    "    # Extract means of the GMM components\n",
    "    means = gmm.means_\n",
    "    covariances = gmm.covariances_\n",
    "        \n",
    "    # Sort means along a principal axis (e.g., first component)\n",
    "    sort_indices = np.argsort(means[:, 0])\n",
    "    means_sorted = means[sort_indices]\n",
    "    covariances_sorted = covariances[sort_indices]\n",
    "    \n",
    "    pcs_ortogonal = []\n",
    "    curve = []\n",
    "    for i, mean in enumerate(means_sorted):\n",
    "        # Compute PCA on the covariance matrix\n",
    "        eigvals, eigvecs = np.linalg.eigh(covariances_sorted[i])\n",
    "        \n",
    "        # Take the second principal component\n",
    "        std_dev0 = np.sqrt(eigvals[0]) \n",
    "        direction0 = eigvecs[:, 0]    \n",
    "        std_dev1 = np.sqrt(eigvals[1]) \n",
    "        direction1 = eigvecs[:, 1]    \n",
    "        \n",
    "        # Ajuste usando o desvio padrão, verificar qual o melhor valor para que tenha um grau de confiança. Isso deve ser documentado na lib\n",
    "        scaled_direction0 = 3 * std_dev0 * direction0\n",
    "        '''\n",
    "        aqui eu multiplico por 14 pois para uma dimensão N a distância deve ser multiplicada por N**0.5, \n",
    "        nesse caso, N = 784 e o resultado é 28. Uso a metade por que será somado dos dois lados dos centroides\n",
    "        '''\n",
    "        scaled_direction1 = 14 * std_dev1 * direction1 \n",
    "        \n",
    "        # Save ellipse data\n",
    "        pcs_ortogonal.append({\n",
    "            'center': mean,\n",
    "            'direction': direction0,\n",
    "            'scaled_direction': scaled_direction0\n",
    "        })\n",
    "        curve.append({\n",
    "            'center': mean,\n",
    "            'direction': direction1,\n",
    "            'scaled_direction': scaled_direction1\n",
    "        })\n",
    "    \n",
    "    return curve, pcs_ortogonal\n",
    "\n",
    "def order_segments(curve_points, ortogonal_components):\n",
    "    '''\n",
    "    Ordena Segmentos e também a componente principal ortogonal  \n",
    "    '''\n",
    "    dim = 784\n",
    "    points_shuffled = np.zeros((2*len(curve_points), dim))\n",
    "    for i, seg in enumerate(curve_points):\n",
    "        center_seg = seg['center']\n",
    "        scaled_direction_seg = seg['scaled_direction']\n",
    "        points_shuffled[2*i] = np.array([center_seg + scaled_direction_seg])\n",
    "        points_shuffled[2*i+1] = np.array([center_seg - scaled_direction_seg])\n",
    "\n",
    "    segments = points_shuffled.reshape(len(points_shuffled) // 2, 2, dim)\n",
    "    # Lista para armazenar a ordem dos segmentos\n",
    "    ordered_segments = []\n",
    "\n",
    "    # Usar o primeiro segmento como ponto inicial\n",
    "    current_segment = segments[0]\n",
    "    ordered_segments.append(current_segment)\n",
    "    ordered_pcs = [ortogonal_components[0]]\n",
    "    current_start, current_end = current_segment\n",
    "\n",
    "    # Criar uma lista dos segmentos restantes\n",
    "    remaining_segments = list(segments[1:])\n",
    "    remaining_pcs = list(ortogonal_components[1:])\n",
    "\n",
    "\n",
    "    while remaining_segments:\n",
    "        # Criar uma lista com todos os extremos dos segmentos restantes\n",
    "        candidates = []\n",
    "        for seg in remaining_segments:\n",
    "            candidates.extend(seg)\n",
    "        candidates = np.array(candidates)\n",
    "        \n",
    "        # Calcular a menor distância entre o ponto final do segmento atual e os candidatos\n",
    "        distances_end = cdist([current_end], candidates)\n",
    "        distances_start = cdist([current_start], candidates)\n",
    "        min_idx_end = np.argmin(distances_end)\n",
    "        min_dist_end = np.min(distances_end)\n",
    "        min_idx_start = np.argmin(distances_start)\n",
    "        min_dist_start = np.min(distances_start)\n",
    "        \n",
    "        if min_dist_start > min_dist_end:  \n",
    "            next_segment_idx = min_idx_end // 2\n",
    "            next_segment = remaining_segments[next_segment_idx]\n",
    "            ordered_pcs.append(remaining_pcs[next_segment_idx])\n",
    "            if min_idx_end % 2 == 0:\n",
    "                current_end = next_segment[1] \n",
    "                ordered_segments.append(next_segment)\n",
    "            else:\n",
    "                current_end = next_segment[0] \n",
    "                ordered_segments.append([next_segment[1], next_segment[0]])\n",
    "                \n",
    "        else:\n",
    "            next_segment_idx = min_idx_start // 2\n",
    "            next_segment = remaining_segments[next_segment_idx]\n",
    "            ordered_pcs.insert(0, remaining_pcs[next_segment_idx])\n",
    "            if min_idx_start % 2 == 0:\n",
    "                current_start = next_segment[1] \n",
    "                ordered_segments.insert(0, [next_segment[1], next_segment[0]])\n",
    "            else:\n",
    "                current_start = next_segment[0] \n",
    "                ordered_segments.insert(0, next_segment)\n",
    "            \n",
    "        # Remover o segmento já utilizado\n",
    "        del remaining_segments[next_segment_idx]\n",
    "        del remaining_pcs[next_segment_idx]\n",
    "\n",
    "    return np.array(ordered_segments), ordered_pcs\n",
    "\n",
    "def calc_ort_line(segment_obj):\n",
    "    return np.array([\n",
    "        segment_obj['center'] + segment_obj['scaled_direction'],\n",
    "        segment_obj['center'] - segment_obj['scaled_direction']\n",
    "    ])\n",
    "\n",
    "def calc_connection_seg(seg_prev, pc_prev, seg_current, pc_current):\n",
    "    pc_norm = (np.linalg.norm(pc_current['scaled_direction']) + np.linalg.norm(pc_prev['scaled_direction'])) / 2\n",
    "    return {\n",
    "        'max_dist': pc_norm,\n",
    "        'seg_points': np.array([seg_prev[1], seg_current[0]]),\n",
    "        'is_conn': True\n",
    "    }   \n",
    "    \n",
    "def extract_final_curve(ordered_segments, ordered_pcs):\n",
    "    final_curve = []\n",
    "    for i, segment in enumerate(ordered_segments):\n",
    "        current_ortogonal_pc = calc_ort_line(ordered_pcs[i])                 \n",
    "        final_curve.append({\n",
    "            'max_dist': np.linalg.norm(current_ortogonal_pc[1] - current_ortogonal_pc[0]),\n",
    "            'ortogonal_pc': current_ortogonal_pc,\n",
    "            'seg_points': segment,\n",
    "            'is_conn': False\n",
    "        }) \n",
    "        if (i>=1 and i < len(ordered_segments)):\n",
    "            conn_seg = calc_connection_seg(ordered_segments[i-1], ordered_pcs[i-1], segment, ordered_pcs[i])        \n",
    "            # append connection segment to the curve \n",
    "            final_curve.append(conn_seg)\n",
    "    return final_curve\n",
    "        \n",
    "'''\n",
    "Aqui, uma ideia interessante seria usarmos a distância de Mahalanobis,\n",
    "utilizando os dados estatísticos estimados pelo gausian mixture para cada segmento.\n",
    "Talvez dê uma medida mais exada da proximidade de um dado a uma curva/segmento e melhore a performance\n",
    "'''    \n",
    "def calc_min_distances(points, segments):\n",
    "    \"\"\"\n",
    "    Calculate the minimum distance from each point to multiple segments and store the segment index.\n",
    "\n",
    "    Parameters:\n",
    "    - points: Array of shape (n, d), where each row is a point in N-dimensional space.\n",
    "    - segments: Array of shape (m, 2, d), where each segment is defined by two points.\n",
    "\n",
    "    Returns:\n",
    "    - min_distances: Array of shape (n,), where each entry is the minimum distance for a point.\n",
    "    - segment_indices: Array of shape (n,), where each entry is the index of the closest segment.\n",
    "    \"\"\"\n",
    "    points = np.array(points)  # Shape: (n, d)\n",
    "    segments = np.array(segments)  # Shape: (m, 2, d)\n",
    "\n",
    "    a = segments[:, 0, :]  # Start points of segments, shape: (m, d)\n",
    "    b = segments[:, 1, :]  # End points of segments, shape: (m, d)\n",
    "\n",
    "    # Vector from A to B (segment direction vectors), shape: (m, d)\n",
    "    ab = b - a\n",
    "\n",
    "    # Squared length of each segment, shape: (m,)\n",
    "    ab_len_sq = np.sum(ab**2, axis=1)\n",
    "\n",
    "    # Expand points to shape (n, m, d)\n",
    "    p_exp = points[:, np.newaxis, :]  # Shape: (n, 1, d)\n",
    "    a_exp = a[np.newaxis, :, :]  # Shape: (1, m, d)\n",
    "    ab_exp = ab[np.newaxis, :, :]  # Shape: (1, m, d)\n",
    "\n",
    "    # Vector from A to P, shape: (n, m, d)\n",
    "    ap = p_exp - a_exp\n",
    "\n",
    "    # Projection factors (t values), shape: (n, m)\n",
    "    t = np.sum(ap * ab_exp, axis=2) / ab_len_sq\n",
    "\n",
    "    # Clamp t to the range [0, 1]\n",
    "    t = np.clip(t, 0, 1)\n",
    "\n",
    "    # Closest points on the segments, shape: (n, m, d)\n",
    "    closest_points = a_exp + t[:, :, np.newaxis] * ab_exp\n",
    "\n",
    "    # Distances from points to the closest points on each segment, shape: (n, m)\n",
    "    distances = np.linalg.norm(p_exp - closest_points, axis=2)\n",
    "\n",
    "    # Minimum distances and corresponding segment indices\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "\n",
    "    return min_distances\n",
    "\n",
    "def gaussian_mixture_principal_curve(data, k):\n",
    "    final_curve = []\n",
    "    curve_points, ortogonal_components = gaussian_mixture_k_segs(data, k)\n",
    "    ordered_segments, ordered_pcs = order_segments(curve_points, ortogonal_components)\n",
    "    final_curve = extract_final_curve(ordered_segments, ordered_pcs)\n",
    "    curve_segments = np.array([seg['seg_points'] for seg in final_curve])\n",
    "    min_distances = calc_min_distances(data, curve_segments)\n",
    "    total_distance = np.sum(min_distances)\n",
    "    print('Distância total para k=', k, '->', total_distance)\n",
    "    return final_curve    \n",
    "\n",
    "import pandas as pd\n",
    "file_path = '../data/digit-recognizer/train.csv' \n",
    "digits_images = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit:  0\n",
      "Initialization 0\n",
      "Initialization did not converge. time lapse 0.19500s\t lower bound -3525940.37194.\n",
      "Initialization 1\n",
      "Initialization did not converge. time lapse 0.18801s\t lower bound -4835607.24137.\n",
      "(784,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\src\\data_analysis\\venv\\lib\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "1-dimensional array given. Array must be at least two-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdigit: \u001b[39m\u001b[38;5;124m'\u001b[39m, digit)\n\u001b[0;32m     51\u001b[0m filtered \u001b[38;5;241m=\u001b[39m digits_images[digits_images[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m digit]\u001b[38;5;241m.\u001b[39mto_numpy()[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m---> 52\u001b[0m curve \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_mixture_principal_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m curves\u001b[38;5;241m.\u001b[39mappend(curve)\n",
      "Cell \u001b[1;32mIn[22], line 227\u001b[0m, in \u001b[0;36mgaussian_mixture_principal_curve\u001b[1;34m(data, k)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgaussian_mixture_principal_curve\u001b[39m(data, k):\n\u001b[0;32m    226\u001b[0m     final_curve \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 227\u001b[0m     curve_points, ortogonal_components \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_mixture_k_segs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     ordered_segments, ordered_pcs \u001b[38;5;241m=\u001b[39m order_segments(curve_points, ortogonal_components)\n\u001b[0;32m    229\u001b[0m     final_curve \u001b[38;5;241m=\u001b[39m extract_final_curve(ordered_segments, ordered_pcs)\n",
      "Cell \u001b[1;32mIn[22], line 41\u001b[0m, in \u001b[0;36mgaussian_mixture_k_segs\u001b[1;34m(data, n_components)\u001b[0m\n\u001b[0;32m     38\u001b[0m curve \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, mean \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(means):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Compute PCA on the covariance matrix\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     eigvals, eigvecs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariances\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(eigvecs)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Take the second principal component\u001b[39;00m\n",
      "File \u001b[1;32mc:\\src\\data_analysis\\venv\\lib\\site-packages\\numpy\\linalg\\_linalg.py:1588\u001b[0m, in \u001b[0;36meigh\u001b[1;34m(a, UPLO)\u001b[0m\n\u001b[0;32m   1585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUPLO argument must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1587\u001b[0m a, wrap \u001b[38;5;241m=\u001b[39m _makearray(a)\n\u001b[1;32m-> 1588\u001b[0m \u001b[43m_assert_stacked_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1589\u001b[0m _assert_stacked_square(a)\n\u001b[0;32m   1590\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m _commonType(a)\n",
      "File \u001b[1;32mc:\\src\\data_analysis\\venv\\lib\\site-packages\\numpy\\linalg\\_linalg.py:195\u001b[0m, in \u001b[0;36m_assert_stacked_2d\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 195\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-dimensional array given. Array must be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    196\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least two-dimensional\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m a\u001b[38;5;241m.\u001b[39mndim)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: 1-dimensional array given. Array must be at least two-dimensional"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data_gaussians():    \n",
    "    # Configurações das gaussianas\n",
    "    np.random.seed(42)  # Para reprodutibilidade\n",
    "    num_gaussians = 5\n",
    "    points_per_gaussian = 1000\n",
    "\n",
    "    # Parâmetros de cada gaussiana (média e covariância)\n",
    "    means = [\n",
    "        [0, 0],\n",
    "        [5, 3],\n",
    "        [10, 2],\n",
    "        [15, 1],\n",
    "    ]\n",
    "\n",
    "    covariances = [\n",
    "        [[2, 3], [0, 0.5]],\n",
    "        [[5,0], [0.2, 0.5]],\n",
    "        [[3, -2], [0, 0]],\n",
    "        [[1.2, 0.6], [0.6, 1]],\n",
    "    ]\n",
    "\n",
    "    # Gerar os dados\n",
    "    data = []\n",
    "    for mean, cov in zip(means, covariances):\n",
    "        gaussian_points = np.random.multivariate_normal(mean, cov, points_per_gaussian)\n",
    "        data.append(gaussian_points)\n",
    "\n",
    "    data = np.vstack(data)\n",
    "\n",
    "    # Adicionar ruído\n",
    "    noise = np.random.normal(0, 0.2, data.shape)\n",
    "    data_with_noise = data + noise\n",
    "    return data_with_noise\n",
    "\n",
    "def load_spiral_data():\n",
    "    import pandas as pd \n",
    "    file_path = '../data/spiral_data.csv'  # Update the path if necessary\n",
    "    spiral_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract x and y coordinates\n",
    "    x_data = spiral_data['x']\n",
    "    y_data = spiral_data['y']\n",
    "    data_with_noise = np.column_stack((x_data, y_data))\n",
    "    return data_with_noise\n",
    "\n",
    "curves = [];\n",
    "for digit in range(10):\n",
    "    print('digit: ', digit)\n",
    "    filtered = digits_images[digits_images[\"label\"] == digit].to_numpy()[:, 1:]\n",
    "    curve = gaussian_mixture_principal_curve(filtered, 7)\n",
    "    curves.append(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curves_segments = []\n",
    "for curve in curves:\n",
    "    segments = [seg['seg_points'] for seg in curve]\n",
    "    curves_segments.append(segments)\n",
    "\n",
    "file_path = '../data/digit-recognizer/test.csv' \n",
    "test_data = pd.read_csv(file_path)\n",
    "data_points = test_data.to_numpy()\n",
    "for curve_idx, curve_segments in enumerate(curves_segments): \n",
    "    min_dists = calc_min_distances(data_points, curve_segments)\n",
    "    column_name = curve_idx\n",
    "    test_data[column_name] = min_dists\n",
    "\n",
    "columns_of_interest = [idx for idx in range(10)]\n",
    "test_data.loc[:, \"label\"] = test_data[columns_of_interest].idxmin(axis=1)\n",
    "answer = test_data[[\"label\"]].copy()  \n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "output_file = \"submission.csv\"\n",
    "answer.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = [idx for idx in range(10)]\n",
    "test_data.loc[:, \"Label\"] = test_data[columns_of_interest].idxmin(axis=1)\n",
    "answer = test_data[[\"Label\"]].copy()  \n",
    "answer[\"ImageId\"] = range(1, len(answer) + 1)  \n",
    "answer = answer[[\"ImageId\", \"Label\"]]\n",
    "\n",
    "# Save to CSV\n",
    "output_file = \"submission.csv\"\n",
    "answer.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
